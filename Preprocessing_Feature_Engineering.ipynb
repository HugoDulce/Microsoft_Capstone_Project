{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# MIND Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries, Imports, Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/christopherstephan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/christopherstephan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import ast\n",
    "import wordninja\n",
    "import warnings\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the path to the training dataset\n",
    "behaviours_train_val_path = \"/Users/christopherstephan/Documents/IE/Term 3/CAPSTONE/Datasets/MINDlarge_train/behaviors.tsv\"  \n",
    "news_train_val_path = \"/Users/christopherstephan/Documents/IE/Term 3/CAPSTONE/Datasets/MINDlarge_train/news.tsv\"\n",
    "\n",
    "#to the testing dataset\n",
    "behaviours_test_path = \"/Users/christopherstephan/Documents/IE/Term 3/CAPSTONE/Datasets/MINDlarge_dev/behaviors.tsv\"  \n",
    "news_test_path = \"/Users/christopherstephan/Documents/IE/Term 3/CAPSTONE/Datasets/MINDlarge_dev/news.tsv\"\n",
    "\n",
    "# Loading TSV file from the specified path\n",
    "behaviours_train_val = pd.read_csv(behaviours_train_val_path, sep=\"\\t\", header=None)\n",
    "news_train = pd.read_csv(news_train_val_path, sep=\"\\t\", header=None)\n",
    "\n",
    "#test files\n",
    "behaviours_test = pd.read_csv(behaviours_test_path, sep=\"\\t\", header=None)\n",
    "news_test = pd.read_csv(news_test_path, sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Column Names for the datasets\n",
    "behaviours_train_val.columns = [\"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"]\n",
    "news_train.columns = [\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"]\n",
    "\n",
    "behaviours_test.columns = [\"impression_id\", \"user_id\", \"time\", \"history\", \"impressions\"]\n",
    "news_test.columns = [\"news_id\", \"category\", \"subcategory\", \"title\", \"abstract\", \"url\", \"title_entities\", \"abstract_entities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q34433\t0.017808\t-0.073256\t0.102521\t-0.059926\t-0.060665\t0.027027\t-0.091728\t-0.003057\t-0.170798\t0.111819\t0.006821\t-0.049873\t-0.050532\t-0.003127\t-0.074472\t-0.115891\t-0.067093\t-0.095272\t0.019178\t-0.083725\t-0.060890\t0.017744\t0.049417\t-0.026014\t-0.048549\t0.017528\t0.044163\t0.022111\t-0.081519\t0.046278\t-0.183939\t-0.063143\t-0.014518\t-0.080644\t-0.099994\t0.085905\t-0.083003\t-0.092844\t-0.216481\t0.125441\t0.179819\t0.036735\t-0.085375\t0.021276\t-0.154971\t0.039009\t0.016059\t0.067725\t-0.148213\t0.158773\t-0.028527\t0.125790\t0.006361\t0.067541\t0.077552\t0.060792\t-0.044511\t-0.005862\t-0.068080\t-0.063204\t-0.094127\t0.115441\t-0.016472\t0.106616\t0.047839\t-0.151805\t-0.111083\t-0.142330\t-0.120680\t-0.050393\t-0.073787\t0.017424\t-0.081620\t0.062599\t-0.022102\t-0.102688\t-0.128149\t-0.075895\t0.095134\t0.000984\t0.010143\t-0.068552\t-0.026573\t0.019735\t-0.000981\t-0.126635\t0.008300\t0.170557\t0.002250\t-0.157175\t-0.077962\t0.013433\t0.045894\t-0.071253\t0.086445\t-0.120466\t0.059235\t-0.071865\t0.058854\t0.024765\t\n",
      "\n",
      "Q41\t-0.063388\t-0.181451\t0.057501\t-0.091254\t-0.076217\t-0.052525\t0.050500\t-0.224871\t-0.018145\t0.030722\t0.064276\t0.073063\t0.039489\t0.159404\t-0.128784\t0.016325\t0.026797\t0.137090\t0.001849\t-0.059103\t0.012091\t0.045418\t0.000591\t0.211337\t-0.034093\t-0.074582\t0.014004\t-0.099355\t0.170144\t0.109376\t-0.014797\t0.071172\t0.080375\t0.045563\t-0.046462\t0.070108\t0.015413\t-0.020874\t-0.170324\t-0.001130\t0.059810\t0.054342\t0.027358\t-0.028995\t-0.224508\t0.066281\t-0.200006\t0.018186\t0.082396\t0.167178\t-0.136239\t0.055134\t-0.080195\t-0.001460\t0.031078\t-0.017084\t-0.091176\t-0.036916\t0.124642\t-0.098185\t-0.054836\t0.152483\t-0.053712\t0.092816\t-0.112044\t-0.072247\t-0.114896\t-0.036541\t-0.186339\t-0.160610\t0.037342\t-0.133474\t0.110080\t0.070678\t-0.005586\t-0.046667\t-0.072010\t0.086424\t0.026165\t0.030561\t0.077888\t-0.117226\t0.211597\t0.112512\t0.079999\t-0.083398\t-0.121117\t0.071751\t-0.017654\t-0.134979\t-0.051949\t0.001861\t0.124535\t-0.151043\t-0.263698\t-0.103607\t0.020007\t-0.101157\t-0.091567\t0.035234\t\n",
      "\n",
      "Q56037\t0.021550\t-0.044888\t-0.027872\t-0.128843\t0.066651\t-0.072159\t0.019879\t-0.183956\t0.080640\t0.069166\t-0.010029\t-0.042466\t-0.045126\t-0.107250\t-0.138343\t-0.041894\t-0.089376\t0.008494\t-0.035197\t-0.019311\t0.044393\t-0.020168\t-0.119964\t0.128805\t0.069869\t-0.118373\t0.049027\t0.017122\t0.023251\t0.187165\t-0.085446\t0.057633\t0.055451\t0.019157\t-0.157557\t0.182949\t-0.064166\t-0.037344\t0.032589\t0.047193\t-0.187472\t-0.021150\t0.080669\t0.015979\t-0.141172\t0.100069\t-0.019638\t0.046921\t-0.033486\t0.153717\t-0.137174\t-0.003173\t0.148454\t-0.055801\t-0.104945\t-0.059554\t0.078643\t-0.164241\t0.074333\t-0.112893\t-0.086524\t0.139236\t0.006486\t0.081578\t-0.033565\t-0.138943\t-0.012322\t-0.125677\t-0.119072\t-0.050572\t-0.117274\t-0.139788\t0.090027\t0.005516\t0.018046\t-0.132586\t-0.114572\t0.174543\t0.035025\t0.015231\t0.122872\t0.007038\t0.019467\t-0.019752\t0.102638\t-0.091577\t-0.056262\t-0.020518\t-0.021345\t-0.080827\t-0.077181\t0.091477\t0.091388\t-0.027993\t-0.112258\t-0.231887\t0.095612\t-0.008997\t-0.157394\t0.088364\t\n",
      "\n",
      "Q1860\t0.060958\t0.069934\t0.015832\t0.079471\t-0.023362\t-0.125007\t-0.043618\t0.134063\t-0.121691\t0.089166\t0.129177\t0.148145\t0.027196\t-0.060636\t0.068760\t0.071959\t0.150306\t-0.099519\t-0.050912\t0.123948\t-0.190319\t-0.096762\t-0.006279\t-0.086810\t-0.026199\t0.017013\t0.043436\t0.058991\t-0.131758\t0.032473\t-0.137706\t-0.009527\t0.085008\t-0.060163\t0.044856\t0.030020\t-0.042486\t-0.098337\t-0.024715\t0.054446\t-0.056230\t0.161813\t-0.106716\t-0.052167\t0.013636\t0.132148\t0.044919\t0.074031\t-0.085483\t-0.083199\t-0.007451\t0.113236\t0.098931\t-0.079819\t-0.026290\t0.051472\t-0.092252\t0.068104\t0.016942\t0.009106\t-0.062264\t-0.001102\t0.050228\t0.016879\t-0.026729\t-0.051632\t-0.083040\t-0.143880\t0.066569\t-0.014793\t-0.047219\t-0.034390\t0.009343\t-0.002716\t-0.094623\t0.000528\t-0.055017\t-0.013458\t-0.038277\t-0.067144\t0.091749\t0.018254\t-0.080948\t0.062850\t0.117076\t-0.115282\t0.050163\t0.091078\t-0.166571\t0.056171\t-0.070713\t-0.014287\t0.013578\t0.099977\t0.012199\t-0.141138\t0.056129\t-0.133727\t0.025795\t0.051448\t\n",
      "\n",
      "Q7737\t-0.021237\t0.176011\t-0.078886\t0.041470\t0.136488\t-0.063177\t-0.013134\t-0.048977\t-0.072779\t0.062055\t0.158583\t0.181071\t-0.080502\t-0.061921\t-0.094876\t0.065664\t-0.021694\t-0.092245\t-0.031412\t-0.025514\t-0.008613\t-0.123416\t0.009160\t-0.035520\t0.012307\t-0.036812\t-0.051794\t0.033258\t0.034445\t0.091374\t-0.060335\t0.012610\t0.108855\t-0.054100\t-0.054447\t0.041506\t-0.030100\t0.002688\t-0.098384\t0.029283\t-0.178362\t-0.015961\t-0.030937\t-0.090683\t0.052224\t0.164800\t0.111740\t0.067277\t-0.146709\t-0.009588\t-0.075620\t0.169547\t-0.060250\t-0.209755\t-0.076133\t0.074479\t-0.238249\t-0.004997\t0.112676\t0.063407\t-0.089622\t-0.000269\t0.053938\t0.011689\t-0.047132\t0.039709\t-0.083715\t-0.130581\t0.011577\t-0.002665\t-0.115150\t-0.014943\t0.090394\t0.149829\t-0.089784\t-0.013170\t-0.067733\t0.041420\t0.114820\t-0.068595\t0.118786\t-0.048465\t-0.246352\t0.217955\t-0.038980\t-0.028710\t-0.061348\t-0.068703\t-0.001809\t-0.073306\t-0.136706\t-0.162670\t-0.101399\t0.039757\t0.123693\t-0.129503\t0.153088\t-0.014275\t-0.113625\t-0.097839\t\n",
      "\n",
      "P31\t-0.073467\t-0.132227\t0.034173\t-0.032769\t0.008289\t-0.107088\t-0.031712\t-0.039581\t0.101882\t-0.106961\t-0.053441\t0.068202\t-0.045584\t-0.140448\t-0.079402\t0.001022\t0.059921\t-0.062510\t0.102848\t0.077947\t-0.063644\t0.050070\t-0.019180\t0.064456\t-0.052222\t0.071078\t-0.036413\t-0.039235\t0.137947\t0.067378\t-0.137468\t0.103482\t0.121755\t-0.006587\t0.063077\t-0.024954\t-0.031300\t-0.056833\t-0.139115\t-0.053570\t0.165815\t-0.022143\t0.006561\t-0.108691\t-0.149139\t0.080943\t0.054542\t-0.034564\t0.082343\t-0.095843\t-0.068758\t0.013850\t-0.025589\t-0.012451\t0.116367\t-0.066981\t-0.006472\t0.136078\t-0.057084\t-0.066427\t-0.035916\t-0.028447\t-0.070395\t-0.052364\t-0.040038\t0.037342\t-0.073347\t0.112529\t0.106537\t0.107426\t0.086297\t0.085833\t0.054393\t0.053187\t0.066242\t0.058507\t-0.047180\t-0.086089\t0.050148\t0.053491\t-0.042370\t-0.110435\t-0.058929\t0.063987\t-0.037393\t-0.057942\t-0.032128\t0.141226\t-0.106979\t0.072183\t-0.045641\t-0.050068\t-0.053686\t-0.045389\t-0.037017\t0.117190\t-0.063597\t-0.056910\t0.058387\t-0.114056\t\n",
      "\n",
      "P21\t-0.078436\t0.108589\t-0.049429\t-0.131355\t0.049300\t-0.094605\t-0.101469\t0.127802\t-0.081245\t0.113759\t-0.171865\t0.049044\t0.141462\t0.117907\t0.040574\t-0.057788\t-0.146715\t-0.085228\t0.020211\t-0.121010\t-0.100422\t-0.081288\t0.031696\t-0.060593\t-0.072303\t0.139442\t-0.133374\t-0.120222\t0.050400\t0.119134\t-0.082276\t0.050498\t-0.108097\t0.045905\t0.118079\t0.069211\t-0.049801\t-0.106901\t0.133158\t-0.065444\t-0.085254\t0.040706\t0.007894\t0.034556\t0.139081\t0.025119\t0.122081\t0.154464\t0.099593\t-0.040400\t0.075233\t0.096659\t0.032061\t-0.154013\t0.085069\t-0.144027\t-0.069370\t0.079479\t0.090121\t-0.154897\t-0.127340\t-0.031645\t-0.093840\t0.123652\t-0.134066\t0.066089\t-0.159245\t0.069276\t0.074938\t-0.129573\t0.076426\t-0.144846\t0.147408\t0.106457\t-0.079138\t0.081598\t-0.132508\t0.102217\t0.117162\t-0.064613\t-0.120491\t-0.075478\t0.013671\t-0.056833\t0.086815\t-0.111679\t0.051020\t0.094203\t-0.092261\t-0.147404\t-0.151203\t0.074341\t-0.030571\t-0.137183\t0.045598\t-0.151155\t-0.066223\t0.057489\t0.130188\t-0.054801\t\n",
      "\n",
      "P106\t-0.052137\t0.052444\t-0.019886\t-0.152309\t0.014144\t-0.180491\t-0.132198\t0.063082\t0.085229\t0.114965\t0.023285\t0.074741\t-0.049949\t-0.082051\t-0.159896\t0.035493\t-0.113929\t-0.111878\t-0.139555\t-0.106166\t-0.011966\t0.154562\t-0.096405\t0.131268\t-0.068482\t0.185240\t-0.072894\t-0.114885\t-0.056082\t0.112026\t0.048216\t0.098032\t-0.098028\t-0.106606\t0.078594\t-0.102013\t-0.001059\t-0.145055\t0.000003\t-0.047816\t0.079029\t-0.078351\t-0.016361\t-0.000218\t-0.038627\t0.057308\t0.036923\t-0.073602\t-0.072402\t0.001785\t-0.002824\t-0.060708\t-0.002136\t-0.017358\t0.059936\t-0.133305\t-0.034796\t-0.075657\t0.147320\t-0.133039\t-0.149887\t0.052375\t0.024344\t0.050036\t-0.146324\t0.075327\t-0.135969\t0.031892\t0.049475\t-0.106037\t0.088477\t-0.185415\t0.105080\t0.107440\t-0.028200\t-0.121917\t-0.165206\t0.026541\t0.125522\t0.080844\t-0.178644\t-0.060746\t-0.078724\t-0.009305\t0.088131\t-0.097797\t-0.155246\t-0.030237\t-0.017188\t0.070897\t-0.088902\t-0.058958\t-0.032021\t-0.147213\t0.082776\t-0.169705\t0.122445\t-0.054737\t0.055321\t0.070961\t\n",
      "\n",
      "P735\t-0.051398\t0.056219\t0.068029\t-0.137717\t-0.030050\t0.061566\t-0.103184\t-0.074124\t-0.118975\t0.122100\t0.090664\t0.050602\t-0.023321\t0.135801\t0.082776\t0.134691\t-0.093377\t-0.100187\t0.060942\t0.058473\t0.065260\t-0.049564\t0.013162\t-0.047667\t-0.054335\t0.123371\t-0.145068\t0.015066\t0.045329\t0.131864\t0.062462\t-0.106206\t-0.117788\t-0.050399\t0.019886\t-0.046332\t0.082650\t0.060583\t0.169631\t0.108123\t-0.030897\t0.046386\t-0.014420\t-0.053038\t0.157436\t-0.021491\t0.087635\t-0.051152\t0.054433\t0.121686\t0.037487\t0.044515\t-0.079680\t-0.114405\t0.029875\t-0.124201\t-0.094803\t0.017489\t0.111024\t-0.108676\t0.011377\t0.143746\t-0.180618\t-0.052341\t-0.118239\t-0.081315\t-0.111308\t0.058716\t-0.111563\t-0.222551\t-0.019004\t-0.102315\t0.269483\t-0.023461\t0.046179\t0.050954\t-0.020268\t-0.085623\t-0.011426\t-0.110763\t-0.158052\t0.104254\t-0.097153\t0.060086\t-0.050420\t-0.121439\t-0.112373\t-0.028001\t0.076174\t-0.132399\t-0.096461\t-0.092234\t0.056870\t0.013640\t0.042696\t0.013683\t-0.021127\t-0.189257\t0.055315\t0.101863\t\n",
      "\n",
      "P108\t0.091231\t0.022526\t0.059349\t-0.141853\t0.035025\t-0.111040\t-0.127337\t0.047645\t-0.172328\t0.090933\t0.022216\t0.079914\t0.043736\t-0.096588\t-0.242773\t-0.039824\t-0.078472\t-0.190807\t-0.075510\t-0.011143\t-0.004291\t-0.109142\t-0.055437\t0.139692\t-0.032522\t0.124695\t-0.054761\t-0.046256\t-0.115983\t0.098595\t-0.087121\t-0.029367\t-0.108338\t-0.021720\t-0.028068\t0.029053\t-0.128703\t-0.103341\t-0.139387\t0.134218\t0.207785\t-0.022484\t0.049616\t0.144433\t-0.102246\t-0.064737\t0.094036\t0.059295\t-0.120209\t0.079042\t0.105340\t0.083430\t-0.007747\t0.033792\t-0.025764\t-0.043842\t0.013634\t-0.119388\t0.001556\t-0.057961\t-0.081745\t0.092388\t-0.053616\t0.095690\t-0.001889\t-0.154143\t-0.177890\t-0.131440\t-0.219218\t-0.111252\t-0.104790\t0.067291\t0.130789\t0.144343\t-0.105937\t-0.070367\t-0.152791\t-0.079241\t0.008933\t0.107746\t0.076368\t-0.071356\t-0.056110\t-0.030554\t-0.087335\t-0.108328\t-0.039597\t0.074101\t0.094331\t-0.088390\t0.026855\t-0.046994\t-0.056248\t-0.146538\t0.121375\t-0.211757\t0.077591\t-0.002200\t-0.053880\t0.140873\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path to files\n",
    "entity_embedding_path = \"/Users/christopherstephan/Documents/IE/Term 3/CAPSTONE/Datasets/MINDlarge_train/entity_embedding.vec\"  \n",
    "relation_embedding_path = \"/Users/christopherstephan/Documents/IE/Term 3/CAPSTONE/Datasets/MINDlarge_train/relation_embedding.vec\"\n",
    "\n",
    "with open(entity_embedding_path, \"r\") as f:\n",
    "    for _ in range(5):  # Read first 5 lines\n",
    "        print(f.readline())\n",
    "\n",
    "with open(relation_embedding_path, \"r\") as f:\n",
    "    for _ in range(5):  # Read first 5 lines\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load entity embeddings\n",
    "entity_embeddings = {}\n",
    "with open(entity_embedding_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        entity = values[0]  # First value is entity name\n",
    "        vector = np.array(values[1:], dtype=np.float32)  # Rest are vector values\n",
    "        entity_embeddings[entity] = vector\n",
    "\n",
    "\n",
    "# Load relation embeddings\n",
    "relation_embeddings = {}\n",
    "with open(relation_embedding_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        entity = values[0]  # First value is entity name\n",
    "        vector = np.array(values[1:], dtype=np.float32)  # Rest are vector values\n",
    "        relation_embeddings[entity] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Val, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the time column to datetime format\n",
    "behaviours_train_val[\"time\"] = pd.to_datetime(behaviours_train_val[\"time\"], format=\"%m/%d/%Y %I:%M:%S %p\")\n",
    "behaviours_test[\"time\"] = pd.to_datetime(behaviours_test[\"time\"], format=\"%m/%d/%Y %I:%M:%S %p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to split data into train and val\n",
    "def split_behaviours(behaviours_train_val, train_ratio=0.82):\n",
    "    # Ensure the DataFrame is sorted by time before splitting\n",
    "    behaviours_train_val = behaviours_train_val.sort_values(by='time')  # Replace 'timestamp' with the actual time column\n",
    "    \n",
    "    # Calculate split index\n",
    "    split_index = int(len(behaviours_train_val) * train_ratio)\n",
    "\n",
    "    # Split the dataset\n",
    "    behaviours_train = behaviours_train_val.iloc[:split_index]\n",
    "    behaviours_val = behaviours_train_val.iloc[split_index:]\n",
    "\n",
    "    return behaviours_train, behaviours_val\n",
    "\n",
    "# Example usage\n",
    "# behaviours = pd.read_csv(\"your_data.csv\")  # Load your dataset\n",
    "behaviours_train, behaviours_val = split_behaviours(behaviours_train_val)\n",
    "\n",
    "# Displaying the split sizes\n",
    "print(f\"Training set size: {len(behaviours_train)}, Validation set size: {len(behaviours_val)}, and the Test set size: {len(behaviours_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(behaviours_train['time'].min())\n",
    "print(behaviours_train['time'].max())\n",
    "print(behaviours_val['time'].min())\n",
    "print(behaviours_val['time'].max())\n",
    "print(behaviours_test['time'].min())\n",
    "print(behaviours_test['time'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning, Preprocessing, Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_behaviours(behaviours):\n",
    "    \"\"\"\n",
    "    Cleans the behaviors dataset by handling missing values and removing duplicates.\n",
    "    \"\"\"\n",
    "    behaviours[\"history\"].fillna(\"No_History\", inplace=True)\n",
    "    behaviours.drop_duplicates(inplace=True)\n",
    "    return behaviours\n",
    "\n",
    "def convert_time(behaviours):\n",
    "    \"\"\"\n",
    "    Converts the 'Time' column to datetime format and extracts the hour.\n",
    "    \"\"\"\n",
    "    behaviours[\"time\"] = pd.to_datetime(behaviours[\"time\"], format=\"%m/%d/%Y %I:%M:%S %p\", errors=\"coerce\")\n",
    "    behaviours[\"hour\"] = behaviours[\"time\"].dt.hour\n",
    "    return behaviours\n",
    "\n",
    "def extract_num_clicks(behaviours):\n",
    "    \"\"\"\n",
    "    Extracts the number of clicks per session based on the 'Impressions' column.\n",
    "    \"\"\"\n",
    "    def count_clicks(impressions):\n",
    "        if not isinstance(impressions, str) or impressions.strip() == \"\":\n",
    "            return 0\n",
    "        return sum([int(i.split('-')[1]) for i in impressions.split() if '-' in i])\n",
    "    \n",
    "    behaviours[\"num_clicks\"] = behaviours[\"impressions\"].apply(count_clicks)\n",
    "    return behaviours\n",
    "\n",
    "def extract_history_length(behaviours):\n",
    "    \"\"\"\n",
    "    Extracts the length of the user's reading history.\n",
    "    \"\"\"\n",
    "    behaviours[\"history_length\"] = behaviours[\"history\"].apply(lambda x: 0 if x == \"No_History\" else len(x.split()))\n",
    "    return behaviours\n",
    "\n",
    "def split_clicked_nonclicked(behaviours):\n",
    "    \"\"\"\n",
    "    Splits the impressions column into separate clicked and non-clicked news lists.\n",
    "    \"\"\"\n",
    "    def split_impressions(impressions):\n",
    "        clicked, non_clicked = [], []\n",
    "        if isinstance(impressions, str) and impressions.strip():\n",
    "            for item in impressions.split():\n",
    "                parts = item.split('-')\n",
    "                if len(parts) == 2:\n",
    "                    news_id, clicked_status = parts\n",
    "                    if clicked_status == \"1\":\n",
    "                        clicked.append(news_id)\n",
    "                    else:\n",
    "                        non_clicked.append(news_id)\n",
    "        return clicked, non_clicked\n",
    "\n",
    "    behaviours[[\"clicked_news\", \"non_clicked_news\"]] = behaviours[\"impressions\"].apply(lambda x: pd.Series(split_impressions(x)))\n",
    "    return behaviours\n",
    "\n",
    "def convert_user_id(behaviours, user_mapping=None):\n",
    "    \"\"\"\n",
    "    Converts 'User ID' into a numerical index for easier model use.\n",
    "    \"\"\"\n",
    "    if user_mapping is None:\n",
    "        user_mapping = {uid: idx for idx, uid in enumerate(behaviours[\"user_id\"].unique())}\n",
    "    behaviours[\"user_index\"] = behaviours[\"user_id\"].map(user_mapping)\n",
    "    return behaviours, user_mapping\n",
    "\n",
    "def compute_user_avg_clicks(behaviours):\n",
    "    \"\"\"\n",
    "    Computes the average number of clicks per user.\n",
    "    \"\"\"\n",
    "    user_click_counts = behaviours.groupby(\"user_id\")[\"num_clicks\"].sum()\n",
    "    behaviours[\"user_avg_clicks\"] = behaviours[\"user_id\"].map(user_click_counts)\n",
    "    return behaviours\n",
    "\n",
    "def compute_recency(behaviours):\n",
    "    \"\"\"\n",
    "    Computes recency of last interaction by measuring the time since the last recorded interaction.\n",
    "    \"\"\"\n",
    "    max_time = behaviours[\"time\"].max()\n",
    "    behaviours[\"recency\"] = (max_time - behaviours[\"time\"]).dt.total_seconds() / 3600\n",
    "    return behaviours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_behaviours(behaviours_train, behaviours_val, behaviours_test):\n",
    "    \"\"\"\n",
    "    Full preprocessing pipeline for the behaviors dataset.\n",
    "    \"\"\"\n",
    "    for dataset in [behaviours_train, behaviours_val, behaviours_test]:\n",
    "        dataset = clean_behaviours(dataset)\n",
    "        dataset = convert_time(dataset)\n",
    "        dataset = extract_num_clicks(dataset)\n",
    "        dataset = extract_history_length(dataset)\n",
    "        dataset = split_clicked_nonclicked(dataset)\n",
    "        dataset = compute_user_avg_clicks(dataset)\n",
    "        dataset = compute_recency(dataset)\n",
    "    \n",
    "    behaviours_train, user_mapping = convert_user_id(behaviours_train)\n",
    "    behaviours_val, _ = convert_user_id(behaviours_val, user_mapping)\n",
    "    behaviours_test, _ = convert_user_id(behaviours_test, user_mapping)\n",
    "    \n",
    "    return behaviours_train, behaviours_val, behaviours_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the full preprocessing pipeline\n",
    "processed_behaviours_train, processed_behaviours_val, processed_behaviours_test  = preprocess_behaviours(behaviours_train, behaviours_val, behaviours_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_behaviours_test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_news(news):\n",
    "    \"\"\"\n",
    "    Cleans the news dataset by handling missing values and duplicates.\n",
    "    \"\"\"\n",
    "    news.dropna(inplace=True)\n",
    "    news.drop_duplicates(inplace=True)\n",
    "    return news\n",
    "\n",
    "def preprocess_text(news):\n",
    "    \"\"\"\n",
    "    Processes text fields in the news dataset by:\n",
    "    - Splitting concatenated words in Category/SubCategory\n",
    "    - Converting text to lowercase\n",
    "    - Removing punctuation\n",
    "    - Removing stopwords\n",
    "    \"\"\"\n",
    "    def split_and_replace(word):\n",
    "        return ' '.join(wordninja.split(word))\n",
    "\n",
    "    news['category'] = news['category'].apply(split_and_replace)\n",
    "    news['subcategory'] = news['subcategory'].apply(split_and_replace)\n",
    "    \n",
    "    columns_to_lower = ['category', 'subcategory', 'title', 'abstract']\n",
    "    news[columns_to_lower] = news[columns_to_lower].applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
    "    \n",
    "    news['content'] = news[['category', 'subcategory', 'title', 'abstract']].apply(' '.join, axis=1)\n",
    "    news['content'] = news['content'].str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    news['content'] = news['content'].apply(lambda text: \" \".join(\n",
    "        [word for word in text.split() if word.lower() not in stop_words]\n",
    "    ))\n",
    "    \n",
    "    return news\n",
    "\n",
    "def extract_word_count(news):\n",
    "    \"\"\"\n",
    "    Adds a new feature 'Content_WC' that represents the word count in the content field.\n",
    "    \"\"\"\n",
    "    news['content_word_count'] = news['content'].str.split().str.len()\n",
    "    return news\n",
    "\n",
    "def extract_entities(news):\n",
    "    \"\"\"\n",
    "    Extracts Wikidata entity IDs from the 'Title Entities' and 'Abstract Entities' columns.\n",
    "    \"\"\"\n",
    "    news['title_entities'] = news['title_entities'].apply(ast.literal_eval)\n",
    "    news['abstract_entities'] = news['abstract_entities'].apply(ast.literal_eval)\n",
    "    \n",
    "    news['title_wikidata_id'] = news['title_entities'].apply(lambda x: ' '.join([d['WikidataId'] for d in x]))\n",
    "    news['abstract_wikidata_id'] = news['abstract_entities'].apply(lambda x: ' '.join([d['WikidataId'] for d in x]))\n",
    "    \n",
    "    news['all_wikidata_ids'] = news['title_wikidata_id'] + ' ' + news['abstract_wikidata_id']\n",
    "    \n",
    "    return news\n",
    "\n",
    "def extract_relations(news):\n",
    "    \"\"\"\n",
    "    Extracts Wikidata relation (property) IDs from the 'Title Entities' and 'Abstract Entities' columns.\n",
    "    \"\"\"\n",
    "    news['title_relations'] = news['title_entities'].apply(lambda x: ' '.join([d['Type'] for d in x]))\n",
    "    news['abstract_relations'] = news['abstract_entities'].apply(lambda x: ' '.join([d['Type'] for d in x]))\n",
    "    \n",
    "    news['all_relation_ids'] = news['title_relations'] + ' ' + news['abstract_relations']\n",
    "    \n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_news(news_train, news_test):\n",
    "    \"\"\"\n",
    "    Full preprocessing pipeline for the news dataset.\n",
    "    \"\"\"\n",
    "    for dataset in [news_train, news_test]:\n",
    "        dataset = clean_news(dataset)\n",
    "        dataset = preprocess_text(dataset)\n",
    "        dataset = extract_word_count(dataset)\n",
    "        dataset = extract_entities(dataset)\n",
    "        dataset = extract_relations(dataset)\n",
    "    \n",
    "    return news_train, news_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_8/hcj6x_gn51q577lb1lp9x6h40000gn/T/ipykernel_5528/1498567931.py:42: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  news[columns_to_lower] = news[columns_to_lower].applymap(lambda x: x.lower() if isinstance(x, str) else x)\n"
     ]
    }
   ],
   "source": [
    "# Run the full preprocessing pipeline\n",
    "processed_news_train, processed_news_test = preprocess_news(news_train, news_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_news_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entity_vectors(news, entity_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the average embedding vector for each news article based on extracted entity IDs.\n",
    "\n",
    "    Parameters:\n",
    "        news (pd.DataFrame): DataFrame containing the news dataset.\n",
    "        entity_embeddings (dict): Dictionary mapping Wikidata IDs to embedding vectors.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated news dataset with 'average_vector' feature.\n",
    "    \"\"\"\n",
    "    def calculate_average_vector(vector_ids, entity_embeddings):\n",
    "        \"\"\"\n",
    "        Helper function to compute the mean vector for a given list of entity IDs.\n",
    "\n",
    "        Parameters:\n",
    "            vector_ids (str): Space-separated string of entity IDs.\n",
    "            entity_embeddings (dict): Dictionary of pre-loaded entity embeddings.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray or np.nan: Averaged vector if entities exist, otherwise NaN.\n",
    "        \"\"\"\n",
    "        vector_ids = vector_ids.split() if isinstance(vector_ids, str) else []\n",
    "        vectors = [entity_embeddings.get(entity) for entity in vector_ids if entity in entity_embeddings]\n",
    "\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            return np.nan  # Keep NaN for missing values\n",
    "\n",
    "    # Apply function to each row in 'All Wikidata IDs' column\n",
    "    news['entity_vector'] = news['all_wikidata_ids'].apply(lambda x: calculate_average_vector(x, entity_embeddings))\n",
    "\n",
    "    return news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "processed_news_train = compute_entity_vectors(processed_news_train, entity_embeddings)\n",
    "processed_news_test = compute_entity_vectors(processed_news_test, entity_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relation_vectors(news, relation_embeddings):\n",
    "    \"\"\"\n",
    "    Computes the average relation embedding vector for each news article.\n",
    "\n",
    "    Parameters:\n",
    "        news (pd.DataFrame): DataFrame containing the news dataset.\n",
    "        relation_embeddings (dict): Dictionary mapping property IDs (Pxxx) to vectors.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated news dataset with 'relation_vector' feature.\n",
    "    \"\"\"\n",
    "    def calculate_relation_vector(relation_ids, relation_embeddings):\n",
    "        relation_ids = relation_ids.split() if isinstance(relation_ids, str) else []\n",
    "        vectors = [relation_embeddings.get(r) for r in relation_ids if r in relation_embeddings]\n",
    "\n",
    "        if vectors:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            return np.nan  # Keep NaN for missing values\n",
    "\n",
    "    # Apply function to each row in 'All Relation IDs' column\n",
    "    news['relation_vector'] = news['all_relation_ids'].apply(lambda x: calculate_relation_vector(x, relation_embeddings))\n",
    "\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function\n",
    "processed_news_train = compute_relation_vectors(processed_news_train, relation_embeddings)\n",
    "processed_news_test = compute_relation_vectors(processed_news_test, relation_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_news_train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Processed Data to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Parquet format\n",
    "processed_news_train.to_parquet(\"processed_news_train.parquet\", index=False)\n",
    "processed_news_test.to_parquet(\"processed_news_test.parquet\", index=False)\n",
    "processed_behaviours_train.to_parquet(\"processed_behaviours_train.parquet\", index=False)\n",
    "processed_behaviours_val.to_parquet(\"processed_behaviours_val.parquet\", index=False)\n",
    "processed_behaviours_test.to_parquet(\"processed_behaviours_test.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pda2",
   "language": "python",
   "name": "pda2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
